{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression from SAS® Viya® on Customer Subscription Length\n",
    "\n",
    "### About the [Churn Dataset](https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset) \n",
    "\n",
    "This dataset was randomly collected from an Iranian telecom company's database over a 12-month period. It comprises 3150 rows, each representing a customer, with information across 13 columns. The dataset includes attributes such as call failures, SMS frequency, number of complaints, distinct calls, subscription length, age group, charge amount, service type, usage duration, status, usage frequency, and Customer Value.\n",
    "\n",
    "All attributes, except for the churn attribute, consist of aggregated data from the first 9 months. The churn labels indicate the customers' status at the end of the 12-month period. The three-month gap is designated for planning purposes.\n",
    "\n",
    "This constitutes a regression task aimed at predicting the subscription length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation with Linear Regression\n",
    "This notebook illustrates how to perform cross-validation (CV) with linear regression as an example. We will primarily utilize sklearn for cross-validation.\n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "\n",
    "1. Building a linear regression model without cross-validation\n",
    "2. Hyperparameter tuning using CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sasviya.ml.linear_model import LinearRegression\n",
    "\n",
    "import warnings # supress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace=f'{os.path.abspath(\"\")}/../data/'\n",
    "subscrlength_df=pd.read_csv(workspace+'churn.csv')\n",
    "subscrlength_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations \n",
    "len(subscrlength_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_X_df = subscrlength_df.select_dtypes(exclude=['object'])\n",
    "numeric_X_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in numeric_X_df.items():\n",
    "    sns.boxplot(y=k, data=numeric_X_df, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots indicate that some variables have outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the correlation between the variables in a correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = subscrlength_df.drop(['Subscription Length','Churn'], axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(X_df.corr().abs(),  annot=True, cmap=\"coolwarm\", annot_kws={\"size\": 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age and Age Group are correlated as expected, while Customer Value and Frequency of SMS are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model Without Cross-Validation\n",
    "Let's build a multiple regression model. First, let's build a vanilla MLR model without any cross-validation etc.\n",
    "\n",
    "For details about using the `LinearRegression` class, see the [LinearRegression documentation](https://documentation.sas.com/?cdcId=workbenchcdc&cdcVersion=default&docsetId=explore&docsetTarget=p0kx8n36nycmj0n1h1o8d3tqfxc3.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test 70-30 split\n",
    "subscrlength_df.rename(columns={'Subscription Length': 'SubscriptionLength'}, inplace=True)\n",
    "X_df = subscrlength_df.drop(['SubscriptionLength', 'Churn'], axis=1)\n",
    "y = subscrlength_df['SubscriptionLength']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using RFE\n",
    "Now, we have 13 predictor features. To build the model using Recursive Feature Elimination (RFE) for feature selection, we need to specify the number of features we want in the final model. RFE then executes a feature elimination algorithm.\n",
    "\n",
    "It is important to note that the number of features to be included in the model is a **hyperparameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of max features\n",
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model with an arbitrary choice of n_features\n",
    "# running RFE with number of features=8\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=8)             \n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples of (feature name, whether selected, ranking)\n",
    "# note that the 'rank' is > 1 for non-selected features\n",
    "\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict prices of X_test\n",
    "y_pred = rfe.predict(X_test)\n",
    "\n",
    "# evaluate the model on test set\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print('r2:', '{:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with another value of RFE\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=11)             \n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# predict prices of X_test\n",
    "y_pred = rfe.predict(X_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print('r2:', '{:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Using Grid Search Cross-Validation\n",
    "A common use of cross-validation is for tuning hyperparameters of a model. The most common technique is what is called grid search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features in X_train\n",
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = [{'n_features_to_select': list(range(1, 13))}]\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm)             \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = rfe, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose the optimal value of number of features and build a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model\n",
    "n_features_optimal = 10\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=n_features_optimal)             \n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# predict prices of X_test\n",
    "y_pred = lm.predict(X_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print('r2:', '{:.4f}'.format(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
