{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForestRegressor from SAS® Viya® on Bike Sharing Demand\n",
    "### Source\n",
    "This example has been adapted from [EDA & Ensemble Model (Top 10 Percentile)](https://www.kaggle.com/code/viveksrinivasan/eda-ensemble-model-top-10-percentile/notebook) by Vivek Srinivasan.\n",
    "\n",
    "### Data Preparation\n",
    "#### About the data set\n",
    "\n",
    "A bicycle sharing system is a system of linked rental locations throughout a metropolitan area that allows individuals to share a pool of bicycles for their use. Bicycles rented from one location may be returned to any other location in the network.  Joining the network and renting or returning bicycles are automated. As of 2022, there are around 3000 cities offering such systems.\n",
    "\n",
    "The data set contains information about the number of bikes rented by different types of users over several years tracked by hour. Additional information about each rental period includes weather information and categorization of the type of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace=f'{os.path.abspath(\"\")}/../data/'\n",
    "bikeData = pd.read_csv(workspace + \"bike_sharing_demand.csv\")\n",
    "bikeData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample of the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information above shows the columns \"hour\", \"weekday\", \"month\", \"season\", \"holiday\", \"workingday\", and \"weather\" as Int64 when they should be a categorical type.  We will coerce those columns from integers into appropriate categorical types. In addition, we will drop the date column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coercing to categorical types and dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeData[\"month\"] = bikeData.month.map({1:\"January\",2:\"February\",3:\"March\",4:\"April\",5:\"May\",6:\"June\",7:\"July\",\n",
    "                                         8:\"August\",9:\"September\",10:\"October\",11:\"November\",12:\"December\"})\n",
    "bikeData[\"weekday\"] = bikeData.weekday.map({0:\"Sunday\",1:\"Monday\",2:\"Tuesday\",3:\"Wednesday\",4:\"Thursday\",5:\"Friday\",6:\"Saturday\"})\n",
    "bikeData[\"season\"] = bikeData.season.map({1: \"Spring\", 2 : \"Summer\", 3 : \"Fall\", 4 :\"Winter\" })\n",
    "bikeData[\"weather\"] = bikeData.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n",
    "                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n",
    "                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n",
    "                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })\n",
    "bikeData[\"workingday\"] = bikeData.workingday.map({0: \"Non-Working Day\", 1: \"Working Day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryVariableList = [\"hour\",\"weekday\",\"month\",\"season\",\"holiday\",\"workingday\",\"weather\"]\n",
    "for var in categoryVariableList:\n",
    "    bikeData[var] = bikeData[var].astype(\"category\")\n",
    "bikeData  = bikeData.drop([\"date\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis\n",
    "\n",
    "#### Visualizing the outliers\n",
    "\n",
    "In order to get a sense of how outliers may be distributed in the data, we will run a series of boxplots for \"count\" by itself and \"count\" versus \"season\", \"hour\", and \"workingday\".\n",
    "\n",
    "The plots allow us to draw the following inferences about the data:\n",
    "* \"count\" contains many outliers beyond the upper quartile limit which skew the distribution towards higher values.\n",
    "* For \"season\", Spring has a relatively lower of \"count\" demonstrated by the lower median value in its boxplot.\n",
    "* The boxplot for \"Hour Of The Day\" has relatively higher median values for 0700, 0800, 1700, and 1800 hours.  This likely matches up with commuter traffic for the start and end of school and office workdays.\n",
    "* \"Working Day\" observations contribute a majority of the outliers, as opposed to \"Non-Working Day\" ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sn.boxplot(data=bikeData,y=\"count\",orient=\"v\",ax=axes[0][0])\n",
    "sn.boxplot(data=bikeData,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\n",
    "sn.boxplot(data=bikeData,y=\"count\",x=\"hour\",orient=\"v\",ax=axes[1][0])\n",
    "sn.boxplot(data=bikeData,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Count',title=\"1. Box Plot On Count\")\n",
    "axes[0][1].set(xlabel='Season', ylabel='Count',title=\"2. Box Plot On Count Across Season\")\n",
    "axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"3. Box Plot On Count Across Hour Of The Day\")\n",
    "axes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"4. Box Plot On Count Across Working Day\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers in the count variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeDataWithoutOutliers = bikeData[np.abs(bikeData[\"count\"]-bikeData[\"count\"].mean())<=(3*bikeData[\"count\"].std())]\n",
    "\n",
    "print (\"Shape of the data with outliers: \",bikeData.shape)\n",
    "print (\"Shape of the data without outliers: \",bikeDataWithoutOutliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "A common method for understanding numeric variables influence dependent variables is through constructing a correlation matrix.  We will specifically examine the correlation of \"count\" versus \"temp\", \"atemp\", \"humidity\", and \"windspeed\".\n",
    "\n",
    "#### Viewing a heatmap\n",
    "From examining the heatmap, we can notice:\n",
    "\n",
    "* \"temp\" and \"humidity\" have a positive and negative correlation with \"count\", respectively. Since the values are noticeably large, \"count\" does not depend much on either.\n",
    "* As \"windspeed\" has a very low correlation with \"count\", it adds little to the analysis. \n",
    "* Since \"atemp\" (measurement of what the temperature \"feels like\") and \"temp\" (thermometer reading) have a strong correlation with each other, including them both would introduce multicollinearity into the analysis. With those definitions, one would expect this relationship and so \"atemp\" will be dropped.\n",
    "* As \"casual\" and \"registered\" are leakage variables in nature, they should also be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = bikeData[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\"]].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(10,8)\n",
    "sn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression plots\n",
    "Regression plots from `seaborn` are a useful way to depict the relationship between two features. We will also view these for \"count\" versus \"temp\", \"atemp\", \"humidity\", and \"windspeed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\n",
    "fig.set_size_inches(16, 5)\n",
    "\n",
    "# Assigning softer colors to each plot\n",
    "color1 = \"#7fbf7b\"  # Light green\n",
    "color2 = \"#af8dc3\"  # Light purple\n",
    "color3 = \"#fdb462\"  # Light orange\n",
    "\n",
    "sn.regplot(x=\"temp\", y=\"count\", data=bikeData,ax=ax1, color=color1)\n",
    "sn.regplot(x=\"windspeed\", y=\"count\", data=bikeData,ax=ax2, color=color2)\n",
    "sn.regplot(x=\"humidity\", y=\"count\", data=bikeData,ax=ax3, color=color3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Distribution of Data\n",
    "As is visible from the figures below, \"count\" is skewed towards the right and not a normal distribution.  Since many machine learning algorithms require a normal distribution, we will need to pursue options to address this.  A common technique is to take a log transformation of \"count\" after removing the outliers. Afterwards, the data distribution looks better but is still not an ideal normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "fig,axes = plt.subplots(ncols=2,nrows=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sn.histplot(x = bikeData[\"count\"], kde = True, ax = axes[0,0])\n",
    "stats.probplot(bikeData[\"count\"], dist='norm', fit=True,plot=axes[0][1] )\n",
    "sn.histplot(x = np.log(bikeDataWithoutOutliers[\"count\"]), kde = True, ax = axes[1,0])\n",
    "stats.probplot(np.log1p(bikeDataWithoutOutliers[\"count\"]), dist='norm', fit=True, plot=axes[1][1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Count Versus Month, Season, Hour, and Weekday\n",
    "The plots help show:\n",
    "* People tend to rent bikes during summer leading June, July, and August to have relatively higher \"count\" (demand).\n",
    "* On weekdays, more people rent bicycles for the 0700, 0800, 1700, and 1800 hours. This overlaps with normal school and office schedules.\n",
    "* The renters during this time are registered users.\n",
    "* On weekend days, more bicycles are rented between 1000 and 1600.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)\n",
    "fig.set_size_inches(12,20)\n",
    "sortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "hueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "\n",
    "monthAggregated = pd.DataFrame(bikeData.groupby(\"month\")[\"count\"].mean()).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by=\"count\",ascending=False)\n",
    "sn.barplot(data=monthSorted,x=\"month\",y=\"count\",ax=ax1,order=sortOrder)\n",
    "ax1.set(xlabel='Month', ylabel='Avearage Count',title=\"Average Count By Month\")\n",
    "\n",
    "hourAggregated = pd.DataFrame(bikeData.groupby([\"hour\",\"season\"],sort=True)[\"count\"].mean()).reset_index()\n",
    "sn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"season\"], data=hourAggregated, join=True,ax=ax2)\n",
    "ax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')\n",
    "\n",
    "hourAggregated = pd.DataFrame(bikeData.groupby([\"hour\",\"weekday\"],sort=True)[\"count\"].mean()).reset_index()\n",
    "sn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"weekday\"],hue_order=hueOrder, data=hourAggregated, join=True,ax=ax3)\n",
    "ax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')\n",
    "\n",
    "hourTransformed = pd.melt(bikeData[[\"hour\",\"casual\",\"registered\"]], id_vars=['hour'], value_vars=['casual', 'registered'])\n",
    "hourAggregated = pd.DataFrame(hourTransformed.groupby([\"hour\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\n",
    "sn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax4)\n",
    "ax4.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Data Discoveries\n",
    "After this visualization, we will integrate our discoveries about the data in our preparation for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeData = pd.read_csv(workspace + \"bike_sharing_demand.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining feature lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalFeatureNames = [\"season\",\"holiday\",\"workingday\",\"weather\",\"weekday\",\"month\",\"year\",\"hour\"]\n",
    "numericalFeatureNames = [\"temp\",\"humidity\",\"windspeed\",\"atemp\"]\n",
    "dropFeatures = [\"casual\",\"date\",\"registered\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping unncessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeData  = bikeData.drop(dropFeatures,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training and test data\n",
    "We split the original data by putting 75% into the training set and 25% into the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bikeData.drop('count', axis=1)\n",
    "Y = bikeData['count']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models - ForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sasviya.ml.tree import ForestRegressor\n",
    "\n",
    "rfModel = ForestRegressor(n_estimators=100,\n",
    "                          max_depth=25,\n",
    "                          min_samples_leaf=1,\n",
    "                          max_features=None,\n",
    "                          criterion='variance',\n",
    "                          random_state=0)\n",
    "\n",
    "rfModel.fit(X_train, y_train, nominals = categoricalFeatureNames)\n",
    "print(\"Forest Regressor -Test Set Score: {:.2f}\".format(rfModel.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
